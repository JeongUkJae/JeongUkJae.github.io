---
layout: post
title: "🎊 PyTorch Ecosystem Day 2021"
tags:
    - pytorch
    - conference
---

PTED 보면서 내용 + 신기한 것들 메모. 부스 형식으로 운영되어서 내가 Gather Town에서 돌아다니는 형식이었고 이야기도 많이 나눌 수 있어서 좋았다.

{% include image.html url="/images/2021/04-22-pted/ecosystem day.png" width=60 %}

한국시간으로는 22일 새벽이고, 미국 서부 기준으로 21일 낮에 시작되었다. 인원수 보니까 대략 300명 좀 안되게 참가한 느낌.

## 시작 키노트

예수 형님의 오프닝으로 시작한다!! 포럼에서 활동하다가 그 이후에 NVIDIA에서 본격적으로 파이토치 개발하기 시작하셨다고.. PyTorch 포럼에서 활동을 많이 하시는 분이라 페이스북 게시글로 오프닝 키노트 소식이 올라오니 저런 반응도 나온다.

{% include image.html url="/images/2021/04-22-pted/jesus.png" width=60 description='https://www.facebook.com/pytorch/posts/2797012123933087' %}

암튼 키노트에서 말하길, 파이토치의 성공요소를 꼽아보면 flexibility, performance, community 정도.

### PyTorch Release

버전이 빨리 올라간다 싶었는데 분기에 한번씩 메이저 버전을 올리는 거였구나.
그리고 Release Candidate도 기간이 정해져있다고 한다. 릴리즈 한달 전에 미리 낸다고.
PyTorch 내부에서 Feature는 3가지로 분류된다고 한다. Stable, Beta, Prototype.

아래는 좀 새로운 릴리즈 소개

* `torch.fx`: toolkit for developers to use to transform `nn.Module`
  * symbolic tracer, IR, python code generation을 포함한다.
  * 나중에 자세히 살펴보자
* beta] `torch.linalg`: np.linalg와 똑같이 동작하게 하려고 한다.
  * `torch.fft`: 이것도 numpy 호환성 같은 느낌으로 만들어졌다고
* torch.profiler
  * GPU monitoring, Tensorboard plugin 추가
  * `torch.autograd.profiler` -> `torch.profiler`로 변경
  * 좋긴 하지만, 사실 TensorFlow에서 잘 지원되던거라 아쉽긴하다.
* Distributed Training
  * beta] pipeline parallelism `torch.distributed.pipeline.sync.Pipe`
    * DDP만으로 스케일링하기 힘드니까..
    * Dev Day 내용이랑 겹치긴 하네
  * beta] DDP Communication Hook: 기본 all reduce를 수정해서 쓸 수 있는 기능
* beta] AMD Gpu binary

### PyTorch Partner Collaborations

* 여러 관련 회사들 나와서 소개해준다.
* ... 스킵
* 중간에 데모가 좀 나오는데 TensorBoard 상의 PyTorch Profiler 화면 잘 만들었구나 싶다
* 와 디즈니에서도 나왔어요
  * 영화 제작에서 검색이나 등등 여러 유틸성 기능을 ML로 잘 해결했다는 느낌인데, 예를 들어 동일 캐릭터가 나오는 씬만 다 긁어모으거나 하는 식
  * 그리고 멀티모달도 굉장히 잘 활용하고 있는데, 거기 데이터가 너무 좋잖아요 ㅋㅋㅋㅋㅋ

### Community Updates

* 포럼이나 GitHub 통계를 보여주면서 많이 성장했다!를 보여줬다.
* Ecosystem은 여기에서 확인하자 <https://pytorch.org/ecosystem/>
* 이런 것 볼 때마다 쉬는 시간에 컨트리뷰팅 하고 싶다는 생각이 많이 든다.

## Posters

본 행사. 포스터 이미지를 부스마다 제공해줬는데, PTD2와는 다르게 공개되지 않다보니까 이미지를 첨부하긴 힘들다. 아래는 관심있는 부스를 돌아다니면서 기록한 내용들이다.

### Compiler & Transform & Production

#### PyTorch development in VS Code

* Microsoft VSCode팀에 소속된 분이 부스 지키고 있었다.
* 아래 기능은 좋아보인다
  * PyTorch Profiler integration
  * Tensorboard integration
  * Multi-dimensional Tensor data exploration
* 관련 블로그 글: <https://devblogs.microsoft.com/python/python-in-visual-studio-code-february-2021-release/>
* TensorBoard plugin은 써봐야겠다.

#### Upcoming features in TorchScript

* 페북 PyTorch Compiler Team
* TorchScript Spec: <https://fb.me/torchscript-spec>
  * 나중에 읽어보자
* Profile-directed Typing for TorchScript
  * 직접 타입 써주는 게 시간도 되게 많이 쓰고, third-party가 중간에 있어버리면 엄청 힘들다.
  * 그래서 Profile-Directed Typing(PTD)라는 기능을 곧 넣을건데, 이게 모델 코드 내에서 함수들의 타입을 다 잡아주는 것.
    * <https://github.com/Instagram/MonkeyType> 참고
  * 지금은 `Model Validity Check` -> `AST Construction` -> `IR Emission` 순서인데, 여기에 `Model Execution` -> `Type Profiling` -> `Type Analysis` 과정 이후 나온 타입을 IR과 같이 내뱉을 예정인 것 같다.
* TorchScript Profiler
  * TorchScript는 Performance Profiling이 항상 없었는데, TorchScript-specific Profiler를 만드는 중이라고 한다.
  * Python code mapping도 가능하게 할 예정
    * Code Mapping -> 이거는 좋다!

#### PyTorch Quantization: FX Graph Mode Quantization

* PyTorch Model Optimization Team
* `torch.fx` 모듈은 진짜 엄청 미는 느낌이다.
  * TensorFlow의 Grappler 기능과 비슷한 기능이라고 생각하고 있는데, TensorFlow는 이 기능이 기본인 것을 보면 Graph 모드의 장점이 크긴 하다보다.
  * 다만 아직은 Prototype 수준이라고
  * <https://pytorch.org/docs/stable/quantization.html#quantization-api-summary> Quantization 문서를 보니 아직은 unstable한 master 문서를 봐달라고 한다.
  * <https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization> 여기를 참고하면 될 듯
* post training quantization은 int8, float16 된다고 하고, QAT에서는 int8 타겟인가 보다. fake quantization 방식으로 하겠지??
* TensorFlow 쪽에서 Quantization 관련 기능이 상당히 아쉬운데, Torch쪽이 잘 된다면 오히려 Onnx나 TorchServe를 활용해서 서빙하는게 더 빠르겠다는 생각이 든다.
* 페이스북 내부 프로덕션 모델에는 이미 적용된 상태인데, 대상 모델은 거의 vision 쪽.
* Next Steps
  * quantized graph가 아직은 더 최적화할 여지가 있다고 한다.
  * debug information을 신경쓸 것이라고 한다. -> 이런 방향은 완전 찬성! ㅋㅋ

#### Accelerate deployment of deep learning models in production with Amazon EC2 Inf1 and TorchServe containers

* 이거 예전에 쓰려고 하던 기능인데 결국 못썼지만, 이제는 잘 되나보다.
* Inferentia chip에 대한 간략한 설명을 가져와보자면
  * fp16, bfloat16, int8같은 타입들 추론도 잘 지원하고 mixed precision도 된다.
  * 1 ~ 16 inferentia chip을 한 인스턴스에 물릴 수 있다고 한다.
  * 4 NeuronCore로 128 TFlops 정도 성능
  * on-chip cache가 많이 달려있고, 8GB DRAM 달려있다.
* `torch_neuron`이라는 패키지로 엄청 쉽게 사용가능하다.
* <https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-pytorch-neuron.html> 이런식으로 사용

#### Torch.fx

* FX: toolkit for writing Python-to-Python transforms
* Symbolic Tracing
  * <https://pytorch.org/docs/stable/fx.html> 문서 봤는데 완전 신기..
  * 아래 코드보면 바로 이해가 간다.

    ```python
    import torch
    # Simple module for demonstration
    class MyModule(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.param = torch.nn.Parameter(torch.rand(3, 4))
            self.linear = torch.nn.Linear(4, 5)

        def forward(self, x):
            return self.linear(x + self.param).clamp(min=0.0, max=1.0)

    module = MyModule()

    from torch.fx import symbolic_trace
    # Symbolic tracing frontend - captures the semantics of the module
    symbolic_traced : torch.fx.GraphModule = symbolic_trace(module)

    # High-level intermediate representation (IR) - Graph representation
    print(symbolic_traced.graph)
    """
    graph(x):
        %param : [#users=1] = self.param
        %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %param), kwargs = {})
        %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})
        %clamp_1 : [#users=1] = call_method[target=clamp](args = (%linear_1,), kwargs = {min: 0.0, max: 1.0})
        return clamp_1
    """

    # Code generation - valid Python code
    print(symbolic_traced.code)
    """
    def forward(self, x):
        param = self.param
        add_1 = x + param;  x = param = None
        linear_1 = self.linear(add_1);  add_1 = None
        clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None
        return clamp_1
    """
    ```

* Graph-based Transformations
  * `fx.Tracer`, `fx.Graph` 사용하면 되게 편하게 변경가능하다고 한다.
  * AST 파싱하듯이 파싱이 되네..?
  * "graph 파싱 후 node를 traverse하면서 `call_function` 이면서 `torch.add` 라면 `torch.mul`로 Op 변경" 같은 일이 가능하다.
  * -> Fused Operator 적용이 훨씬 쉬워질 듯
  * <https://pytorch.org/docs/stable/fx.html#direct-graph-manipulation> 문서는 여기 섹션 참고
* Python code generation
  * 위 코드에서 보이듯 code generation이 바로 된다.
* FX가 어디 쓰일까
  * 곧 나올 graph-mode quantization에 쓰이는 중이다.
  * 인텔에서 ngraph(<https://github.com/NervanaSystems/ngraph>)라는 라이브러리를 개발하는 것으로 아는데, ngraph 같은 기능을 바로 지원할 수도 있겠다라는 생각이 든다.
* 예시는 <https://github.com/pytorch/examples/tree/master/fx> 참고

#### AI Model Efficiency Toolkit (AIMET)

* Qualcomm에서 개발하는 라이브러리. <https://github.com/quic/aimet>
* 나중에 관심생기면 봐야지 ㅎㅎ

### Database & AI Accelerators

#### Enabling PyTorch on AMD Instinct™ GPUs with the AMD ROCm™ Open Software Platform

* 되게 예전부터 있었던 프로젝트로 기억하는데, "M1에서 ML 모델 돌리는 것 가능하냐!!"라는 얘기나오면서 막 같이 소식이 나왔던 걸로 기억한다.
* <https://github.com/pytorch/pytorch/tree/master/torch/utils/hipify> 여기서 code conversion하는 것 확인 가능하다.
* 자세히는 관심없어서 여기까지만..

### Distributed Training

#### DeepSpeed: Shattering barriers of deep learning speed & scale

* DeepSpeed Team
* <https://arxiv.org/abs/2104.07857>
* <https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/>
* ZeRO Infinity 구현.
  * V100 16장으로 1T 모델 학습 가능. (기존: V100 1024장)
  * 1-bit Adam/LAMB으로 비슷한 수렴에 5x 낮은 communication volume 달성
  * Azure ML, Hugging Face, PyTorch Lightning으로 쉽게 개발 가능
* 대형 모델은 무조건 Torch로 가야겠구나.. 논문은 따로 읽어봐야지

#### FairScale - A general purpose modular PyTorch library for high performance and large scale training

* Facebook AI Research에서 개발하는 FairScale
* torchgpipe, AdaScale, ZeRO, ZeRO-Offload 등등을 참고했는데, 그걸 보아 DeepSpeed 등에서 많은 기능을 가져온 것으로 보인다.
* <https://github.com/facebookresearch/fairscale>
* DeepSpeed는 여러모로 시도했던 분들에게 약간의 장벽이 있다고 들었는데, FairScale은 그런 점에서 좀 덜하지 않을까라는 기대가 있다.
* 핵심 기능은 아래정도
  * Pipeline Parallelism
  * AdaScale & Mixed Precision Training
  * Optimizer Sharding, Gradient Sharding, OffloadModel
* 사용법은 확실히 더 간단해 보인다. -> Torch쪽과 아무래도 더 가까운 회사일테니 기대해볼 수 있지 않을까?

#### Accelerate PyTorch large model training with ONNX Runtime: just add one line of code

* Microsoft AI
* `torch-ort`를 pip으로 설치후 `torch_ort.ORTModule`로 `nn.Module`을 감싸주기만 하면 되는 간단한 사용법이다.
* DeepSpeed와 연동이 가능하다.
* T5를 학습할 때 벤치마크해보니 DeepSpeed ZeRO1 + ORTModule을 사용하면 DeepSpeed ZeRO1만 사용했을 때보다 12%정도 성능향상이 있다고.
* 사용법이 엄청 간단해서 써볼만 할 것 같다.
* ONNX가 근데 PyTorch 쪽이랑 엄청 뭔가를 많이 하긴 하네요.

### Frontend & Experiment Manager

#### Accelerate PyTorch with IPEX and oneDNN using Intel BF16 Technology

* Intel, Facebook이 같이 쓰여져 있었다.
* PyTorch 쪽도 intel과의 협력으로 oneAPI 통합이 되었나보다.
* AVX512 & bfloat16 지원이 메인인 것으로 보인다. (3세대 제온 프로세서 기준)
* `intel_pytorch_extension` 패키지 <https://github.com/intel/intel-extension-for-pytorch>
* BERT Large 기준 1.41배 정도 성능향상

#### Hydra Framework

* 요즘 계속 쓰는 라이브러리라 기본 사용법은 패스. 좋긴 진짜 좋다.
  * 가끔 interpolation이나 anchor같은 기능이 맘대로 안될 때도 있지만 빨리빨리 고쳐지는 편
* <https://hydra.cc>
* Tab Completion이 가능한지 몰랐네..

### NLP & Multimodal, RL & Time Series

#### Rolling out Transformers with TorchScript and Inferentia

* Autodesk에서 conversational bot을 support용으로 운영. 그걸 Inferentia로 추론
* BERT나 그 관련 모델로 Sequence Classification Head 붙여서 파인튜닝.
* AWS Inferentia, PyTorch로 4.9x 성능 향상
* Neuron SDK로 잘 감싸면 엄청 좋음

#### MMF: A modular framework for multimodal research

* <https://github.com/facebookresearch/mmf>
* Visual QA 같은 거 쉽게 할 수 있겠다.
* Text, Image, Audio, Video 지원된다고.
* 시간날 때 살펴봐야겠다.

#### RL Based Performance Optimization of Deep Neural Networks

* Facebook AI
* 기존엔 Solution Space에서 Candidates를 골라서 Evaluation 하는 것을 반복하면서 최종 솔루션을 냈는데, RL로 학습된 Policy에 따라 바로 NN을 최적화한다.
* 이거 DevDay에 관련 세션있었던 것 같은데..
* 아마 이해가 잘 안갈 것 같아서 써두면, 연산 최적화다.
* <https://arxiv.org/abs/2011.14486> 나중에 읽어보면 재밌겠다 ㅋㅋ
* AutoTVM보다 월등한 성능

#### The Hugging Face Ecosystem

* 이거 breakout session으로 들어서 패스!

### Performance & Profiler

#### Introducing New PyTorch Profiler

* 요거 오프닝 톡으로 들어서 패스!

#### TRTorch: A Compiler for TorchScript Targeting NVIDIA GPUs with TensorRT

* NVIDIA
* TensorRT + TorchScript
* `trtorchc` 라는 명령어를 써놓았길래 뭐지..? 했더니 TensorRt TORCH Compiler를 줄인듯
* jit으로 trace된 모델 파일을 compile하면 됨
* A100에서 TensorRT를 사용했을 때 object detection model(<https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/>요거인듯) 기준으로 FP32 JIT 대비 FP16성능이 14x 높음
* Post Training Quantization도 써놓았는데, 이건 뭐 예전에도 잘 되었었던 것 같다.
* 근데 한 1년 반쯤 전에 TensorRT를 쓰려고 헀다가 Variable Length에서 잘 동작하지 않아서 힘들었던 기억이 있다. 잘 되면 성능이 무조건 좋아지기 때문에 써보고 싶긴 한데, 지금 어떨지는 확인해봐야겠다.

### Platforms & Ops & Tools

#### FairTorch: Aspiring to Mitigate the Unfairness of Machine Learning Models

* 이거 PyTorch Global Hackathon에서 본 것 같은데..? 이런 프로젝트 이어가는 사람들도 대단하고 계속해서 이런 사람들을 커뮤니티에 끌고오는 PyTorch도 대단하다.
* <https://github.com/wbawakate/fairtorch>
* 굉장히 민감한 피쳐들(race, gender) 등등을 들어간 추론 결과에서 최대한 통계적 차이를 없애려고 한다.
* 쓸 것 같긴 않아도 굉장히 재밌는 프로젝트 같이서 정리

### Vision

#### PyTorchVideo: A Deep Learning Library for Video Understanding

* <https://pytorchvideo.org>
* FAIR 내부에서 만든건가보다
* 일단 PyTorch 구현에 비해 빠르다는데, 내부 최적화를 좀 잘 해놓은 것 같다. <- 나중에 살펴봐야지
* 비디오 쪽도 배경지식처럼 알아두면 유용할 것 같아서 그냥 내용만 읽어봤다.

#### PyTorch 3D: Fast, Flexible 3D Deep Learning

* Facebook AI에서 개발
* <https://pytorch3d.org>
* <https://arxiv.org/abs/2007.08501> 관련 논문인데 읽어보면 좋을 듯
* 회사에서 쓰임새를 찾긴 어렵고, 당장 쓸 것 같지도 않지만, 언젠가 3D 데이터 다뤄보고 싶어서 읽어봤다.

#### CompressAI: a Research Library & Evaluation Platform for End-to-End Compression

* <https://github.com/InterDigitalInc/CompressAI>
* AutoEncoder로 encoding(compress), decoding(decompress)하는 것을 라이브러리화 했나보다.
* TF 유저면 <https://github.com/tensorflow/compression> 참고하면 될 듯
* End-to-End로 jpeg보다 N배작은 이미지를 주고 받을 수 있다면 모바일에서 써볼만 하지 않을까..?
* 이거 역시 나중에 볼 용도로 메모

## Breakout Session

### Huggingface Ecosystem

* 첫번째 시간에서 제일 재밌어보였다.
* 모델 허브 2.0
* HuggingFace Datasets
  * 이거 좋긴한데, 차라리 PyTorch만 타겟하고 맘먹고 지원했으면 더 좋았겠다라는 느낌
  * <https://huggingface.co/datasets?filter=languages%3Ako>
* `huggingface_hub` 라는 라이브러리 참고해보자
* HuggingFace Ecosystem
  * Hub에서 불러와서 Transformers + Datastes + Tokenizers로 학습시키고, Model hub로 다시 올려서 Inference API로 사용하게 한다.
  * 데이터셋만 잘 있으면 다 통합가능하긴 할 듯.
  * AWS EC2 Inf1도 지원..?
* 걱정되는 점
  * 프로덕션 환경에서 조금 특이한 데이터셋 넣을 때 이제 끝없는 개조가 되지 않을까..?
  * -> 확실히 지금까지는 다시 짜는게 빠르다고 생각했고 그랬는데, 이제는 확인해봐야 할 것 같다.
* accelerate가 나오는데 조금은.. 부정적..
  * `이걸 왜 torch 쪽에서 처리하지 않고 라이브러리 쪽에서 처리해야하지??`라는 생각이 드는데, 1~2년사이에 파이토치 내부에서 변화가 생기게 되면 끝없는 레거시 + 의존성 버전의 지옥이 시작되지 않을까? -> 물론 실험 코드는 이런거 적극적으로 써도 상관없다고 생각한다.
  * DeepSpeed, FairScale 급이 아니면 안써도 괜찮지 않을까

### Constrained Optimization in PyTorch 1.9 Through Parametrizations

<https://pytorch.org/docs/master/generated/torch.nn.utils.parametrize.register_parametrization.html>

제목이 신기해서 들어가보았다. 위 모듈 설명인데, weight에 constraint를 걸어서 optimize하는 내용이고, 특정 분야에서 쓸만한 내용같아 보인다.
세션에서 나온 예시는 symmetric한 weight를 사용하도록 작성했다.

아래같은 방식으로 사용가능.

```python
import torch
from torch.nn.utils import parametrize

class Symmetric(torch.nn.Module):
    def forward(self, X):
        return X.triu() + X.triu(1).T

linear = torch.nn.Linear(5, 5)
parametrize.register_parametrization(m, "weight", Symmetric())

assert torch.allclose(linear.weight, linear.weight.T)
```

구현은 property로 구현한 듯 하다. type은 텐서가 아니라 property object가 나온다.
원래 weight은 `.original` suffix가 붙은채로 다른 곳에 할당된다.

caching은 `contextmanager`를 통해 수행하는구나.

수학적인 베이스가 강한 사람이 다양한 쪽으로 사용가능해보이는데, 나는 어떻게 활용할지 모르겠다.

### Avalanche: an End-to-End Library for Continual Learning based on PyTorch

이런게 있구나.. 나중에 살펴봐야지 <https://avalanche.continualai.org>

## 후기

다음날 회사는 휴가를 써서 너무 피곤해서 못 들을 때까지 맘편하게 들었다. 그나저나 이번에도 어쩌다가 초대받았는데, 사실 내가 뭘로 초대받은 건지 궁금하다. 주위에 참여하는 사람이 거의 없기도 하고.

그래도 옆에 Piotr Bialecki 같은 분이 있고, 저 옆에 Thomas Wolf같은 분이 행사 참가하고 있는 것을 본다는 게 나에게는 좋은 경험이기 떄문에 초대 받은 것이 많이 감사하다. 추가로 네트워킹 세션에서 어떻게 PyTorch 기여를 시작했었는지 썰 푸는 것도 듣고, 조언도 듣고 했으니..

중간중간에 여러 회사에서 나온 사람들과 이야기해보는데, 어쩌다가 한 새벽 세시쯤에 회사가 어딘지 얘기가 나와서 한국에 있는 회사라고 했고 몇시인지 묻길래 대답했더니 많이 놀라는 게 웃겼다.
온라인 컨퍼런스는 당연히 새벽이라 생각했는데, 생각해보니 주최측에서는 낮시간이겠거니 싶어서 납득이 되기도 하고.
그렇게 스몰톡도 나누고 여러 라이브러리 사용 케이스 설명듣고, 회사에서 어떻게 쓰는지 썼는지 이야기 나누는데 정말 신기했다.

중간에 접속이 안되어서 엄청 당황했는데, Gather Town 장애이긴 하지만, 이렇게 큰 행사도 다운되기도 하는구나 싶었다. 그리고 PyTorch Dev Day 2020때보다 네트워킹 세션도 더 잘 되어있는 것을 보니 진짜 행사 준비한 분들 대단하다.. 👏

* [PyTorch Developers Day 2020 행사 포스트](/posts/ptd2-2020/)

{% include image.html url="/images/2021/04-22-pted/gather.png" width=50 description='그리고 중간에 아는 사람도 만났다' %}
