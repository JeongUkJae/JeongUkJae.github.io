---
layout: post
title: "신경망/퍼셉트론"
tags:
  - book
---

[정석으로 배우는 딥러닝 - 교보문고 링크](https://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791158390822&orderClick=4bb)를 읽으면서 공부한 내용을 정리하였습니다.

## Neural Network

### 기본개념

 기계학습의 Neural Network(신경망)는 인간의 뇌의 구조에서 본따 만든 구조이다. 인간의 뇌 구조를 모방하여 만든 이 신경망은 뉴런이라는 기본 단위로 이루어진다. 인간의 뇌가 뉴런의 네트워크이고, 기계학습의 신경망 또한 뉴런의 네트워크이다. 인간의 뇌에서는 뉴런과 뉴런 사이에는 전기 신호가 오가는데, 특정 임계값을 넘을 때 발화한다고 한다. 기계학습의 신경망은 뉴런의 입력으로 들어오는 값을 특정 함수에 따라 출력값을 만들어낸다.

 신경망은 계층 구조를 이루고 이쓴데, 각각의 뉴런의 신호가 네트워크를 따라 계층을 따라 최종적으로 응답하는 계층에 도달하여 결과값을 만들어내게 된다.

### 모델

 단순하게 모델로 만들어보면 아래와 같다.

 ![NN]({{ site.url }}/images/2018-05-20-nn/NN.png)

 이전의 뉴런에 x1, x2의 입력값이 들어와서 다음 뉴런으로 전해진다고 하자. 각각의 뉴런은 결합강도가 다르다. 우리는 이 결합강도를 weight라고 부른다. 실제로 1의 값이 전달되어야 하지만, 결합강도가 0.3정도밖에 되지 않아 0.3의 값만 전달될 수 있다. 이전의 뉴런에 의해 전달되는 값을 식으로 쓰면 아래와 같다.

 $$ w_1x_1 + w_2x_2 $$

 이 값이 특정 임계값을 넘을 때 발화한다고 하였으므로, 임곗값을 설정할 차례이다. 우리는 이 값을 $$\theta$$로 설정하자.

 $$ w_1x_1 + w_2x_2 - \theta $$

 임곗값을 넘을 때 발화를 하고, 임곗값을 넘지 못할 때 발화를 하지 못하는 것을 식으로 쓰면 아래와 같다.

 $$ y = \begin{cases}
      1 & (w_1x_1 + w_2x_2 - \theta \geq 0) \\
      0 & (w_1x_1 + w_2x_2 - \theta < 0) \\
    \end{cases} $$

 위의 경우에서 우리가 뉴런에 정해주어야 하는 값은 $$w_1, w_2, \theta$$이다. 우리가 이 값을 설정에서 한번에 맞지 않는다면, 틀린만큼 수정해가며 적정한 값을 찾아가야 하는데, 이를 '오차정정학습법'이라고 한다.

### 단순 퍼셉트론

 이제 하나의 퍼셉트론에 많은 수의 뉴런이 입력으로 들어온다고 하자. 그럼 아래처럼 쓸 수 있다.

 $$ output = w_1x_1 + w_2x_2 + ... + w_nx_n - \theta $$

 이를 행렬로 간단하게 표현할 수 있다. 아래처럼 x, w 행렬을 정의하자.

 $$ x = \begin{pmatrix} x_1 \\ x_2 \\ ... \\ x_n \end{pmatrix}, w = \begin{pmatrix} w_1 \\ w_2 \\ ... \\ w_n \end{pmatrix} $$

 위처럼 정의하면 아래처럼 고쳐쓸 수 있다.

 $$ output = w^Tx - \theta $$

 우리는 임곗값을 bias라는 말로 고쳐쓸 수 있다. $$ b = - \theta $$로 적으면, 아래처럼 쓸 수 있다.

 $$ output = W^Tx + b $$

 그리고 $$ f(x) = \begin{cases}
      1 & (x \geq 0) \\
      0 & (x < 0) \\
    \end{cases} $$로 정의하면, 아래처럼 쓸 수 있다.

 $$ y = f(output) = f(W^Tx + b) $$

### 로지스틱 회귀

 이 때 활성함수에 대한 개념의 필요성이 나온다. 위 수식의 $$f$$를 활성함수(Activation Function)로 볼 수 있는데, 무조건 1과 0으로만 나오는 결과보다, 일정한 퍼센트, 비율 정도로 값을 뽑아내고 싶은 경우, 활성함수로 다른 함수를 사용할 수 있다. 대표적인 함수가 sigmoid이다.

 $$ \sigma (x) = \frac 1 {1 + e^{-x}} $$

 위가 sigmoid 함수 식이고 아래가 그 그래프이다.

 ![sigmoid function]({{ site.url }}/images/2018-05-20-nn/sigmoid.png)

 이 sigmoid 함수는 로지스틱 모형과 식이 같기 때문에 sigmoid 함수를 activation function으로 활용하면 로지스틱 회귀로 부를 수 있다고도 한다.

 이 sigmoid 함수를 사용하는 까닭은 sigmoid 함수를 미분해보면 알 수 있다. 미분식에 자기 자신이 다시 등장하는 형태로, 계산이 간편해 자주 이용한다.

 $$ \sigma^\prime (x) = \sigma (x) (1 - \sigma (x)) $$

 이 로지스틱 회귀는 단순 퍼셉트론과 달리 확률적인 분류라고 한다. 단순 퍼셉트론은 1, 0으로 확실한 값이 나오는 반면, 로지스틱 회귀는 0에서 1사이의 값이 나온다.
