---
title: "최근 읽은 논문/아티클 정리"
layout: post
tags:
  - paper
---

.

## [Triplet Loss for Knowledge Distillation](https://arxiv.org/pdf/2004.08116.pdf)

* IJCNN 2020
* Metric Learning에서의 KD라 보면 된다.
* Triplet Loss를 anchor에 대해서 positive를 가까이 만들고 negative를 멀게 만드는데, 이 논문에서는 knowledge distillation과 같이 수행하기 위해 조금 변경했다.
* 아래처럼 바꿈

{% include image.html url="/images/2020/최근 논문 정리/tlkd.png" width=60 %}

* 실제로 teacher의 embedding space를 따서 student를 만들어야 하는 경우 꽤 좋은 방법인 것 같아서 아이디어만 정리

## [Weight Distillation: Transferring the Knowledge in Neural Network Parameters](https://arxiv.org/pdf/2009.09152.pdf)

* 아래의 개념이 너무 신기해서 정리

{% include image.html url="/images/2020/최근 논문 정리/wd.png" %}

* teacher의 soft label에 대해 학습을 하는 것이 아니라 parameter generator를 학습함
* 학습은 페이즈가 2개인데,
  * 먼저 parameter generator를 학습한다.
  * 그리고 그를 통해 얻은 student를 학습한다.

{% include image.html url="/images/2020/최근 논문 정리/wd-result.png" %}

* 결과를 보면 꽤 괜찮다. KD에 비해 좀 더 robust하고, 성능이 덜 떨어진다.
* NMT에 대해서 성능을 테스트했지만, transfer learning의 관점에서 본다면 좋은 시도 같은데 한번 테스트해볼까..?
