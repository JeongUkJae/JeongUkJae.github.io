---
title: "최근 읽은 논문 정리"
layout: post
tags:
  - paper
---

경량화 & 작은 모델에서 잘 돌아갈 수 있는 기법을 위주로 논문을 살펴보았다.

## [AIN: Fast and Accurate Sequence Labeling with Approximate Inference Network](https://arxiv.org/pdf/2009.08229.pdf)

* EMNLP 2020
* CRF는 sequential computation이 존재하기 때문에 parallelization이 힘들다.
* Sequence Labeling 문제(NER, POS Tagging)에서 BiLSTM - CRF가 가장 좋은 모델이라 알려져있다.
  * CRF에 대해서는 [Lafferty et al., 2001](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)를 참고가능할 듯
* 여기서 속도를 향상하는 방법은 아래정도
  * BiLSTM을 CNN으로 교체 ([Strubell et al., 2017](https://www.aclweb.org/anthology/D17-1283/))
  * Encoder에 해당하는 Embedding - BiLSTM을 작은 모델로 Distillation ([Tsai et al., 2019](https://www.aclweb.org/anthology/D19-1374/))
* 하지만 위 두 방법 모두 CRF를 교체한 것이 아니라 다른 모델 부분을 교체한 것인데, 정작 병목은 CRF에도 있기 떄문에 CRF를 교체
* Mean-Field Variational Inference (MFVI)를 적용해서 linear chain CRF를 교체
* binary feature를 사용하는 CRF를 transition score만을 저장하는 matrix하나와 input sentence의 contextual representation의 matmul 결과 값을 쓴다.
* CRF와 비슷한 성능을 보이면서 BiLSTM, 128 words 길이에서 4배 이상빠른 속도를 보여준다.

## [Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference](https://arxiv.org/pdf/2009.09364.pdf)

* MHA가 attention collapse라는 문제를 상당히 많이 겪는다.
  * 서로 다른 head가 같은 feature만을 잡으려는 현상이고 이게 모델 capacity에 비해 안좋은 성능의 원인이 된다.
* 그래서 이 논문에서는 Bayesian 관점에서 MHA를 분석한다. 그리고 particle-optimization sampling technique을 사용하여 MHA의 성능을 향상시키는 방법을 제시한다.
* 자세한 원리는 잘 모르겠지만, 이 논문의 핵심은 아래 정도로 보인다.
  * NLL Loss를 예시로 들면 gradient가 어느정도 uniform하게 나올 것이다. 그래서 Head를 아래와 같은 식으로 업데이트한다.

{% include image.html url="/images/2020/최근 논문 정리/eq34.png" width=60 %}

* approximation error가 있을 수 있다고 하니까 이건 주의하자 ([Zhang et al., 2018](https://arxiv.org/abs/1809.01293))
* 대신 큰 데이터셋에서 잘되는데 성능은 확실히 오른다.
