---
title: "최근 읽은 논문 정리"
layout: post
tags:
  - paper
---

.

## [Small and Practical BERT Models for Sequence Labeling](https://www.aclweb.org/anthology/D19-1374.pdf)

* EMNLP 2019
* public multilingual BERT checkpoint에서 시작해서 마지막엔 6배 작고 27배 빠르면서 정확도는 multilingual baseline 보다 높은 모델을 얻었다.
* single CPU에서 돌릴 수 있다.
* 3-layer BERT를 Teacher에 대해 Unlabeled Corpus로 Distillation을 하고, student모델을 labeled data에 대해 조금 더 finetuning을 함
  * Sequence Labeling 문제이기 때문에 일반 Unlabeld Corpus에서는 Teacher Logit을 답으로 이용
* 추론 속도가 얼마나 나오는지는 나와있지 않은데, BERT가 Xeon CPU에서 230ms가 걸렸고 27배 빠르다고 하니까 대략 8.5ms 정도 걸린 것 같다.

## [PRADO: Projection Attention Networks for Document Classification On-Device](https://www.aclweb.org/anthology/D19-1506.pdf)

* EMNLP-IJCNLP, 2019
* 모델 크기도 작은데 (~200KB) CNN, LSTM보다 좋은 성능을 낸다.
* 우선 long text classification을 풀기 위함인 듯 함.
* 현재까지는 통계학적 방법 + LSTM/CNN 활용등이 있었고, 최근에는 SGNN(self governing neural network)이나 SGNN++같은게 있었다. ((Ravi and Kozareva, [2018](https://www.aclweb.org/anthology/D18-1105/), [2019](https://www.aclweb.org/anthology/P19-1368/)), (Ravi, [2017](https://arxiv.org/abs/1708.00630), [2019](http://proceedings.mlr.press/v97/ravi19a/ravi19a.pdf)))
*
